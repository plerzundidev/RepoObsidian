# üéØ Preguntas de Entrevista - Data Engineering

## 1. üìã SQL (Nivel T√©cnico)

### B√°sico
**P: ¬øCu√°l es la diferencia entre WHERE y HAVING?**
> WHERE filtra filas **antes** de la agregaci√≥n. HAVING filtra **despu√©s** del GROUP BY.

**P: ¬øDiferencia entre DELETE, TRUNCATE y DROP?**
> - `DELETE`: Borra filas, se puede filtrar, genera logs, se puede rollback
> - `TRUNCATE`: Borra todas las filas, m√°s r√°pido, no genera logs por fila
> - `DROP`: Elimina la tabla completamente (estructura + datos)

### Intermedio
**P: Explica las Window Functions con un ejemplo.**
> Las Window Functions calculan un valor por fila basado en un grupo de filas relacionadas, sin colapsar el resultado como GROUP BY.
> ```sql
> SELECT nombre, departamento, salario,
>        RANK() OVER (PARTITION BY departamento ORDER BY salario DESC) AS ranking
> FROM empleados;
> ```
> Esto calcula el ranking salarial dentro de cada departamento sin eliminar filas.

**P: ¬øQu√© es un CTE recursivo? ¬øCu√°ndo lo usar√≠as?**
> Un CTE que se referencia a s√≠ mismo. √ötil para jerarqu√≠as (organigramas, categor√≠as anidadas, grafos).

### Avanzado
**P: ¬øC√≥mo optimizar√≠as una query que tarda 30 minutos?**
> 1. `EXPLAIN ANALYZE` para ver el plan de ejecuci√≥n
> 2. Verificar si hay full table scan ‚Üí crear √≠ndices
> 3. Revisar JOINs innecesarios o mal ordenados
> 4. Eliminar `SELECT *`, solo columnas necesarias
> 5. Particionar la tabla si es muy grande
> 6. Considerar vistas materializadas para queries recurrentes

---

## 2. üêç Python y Spark

**P: ¬øDiferencia entre Pandas y PySpark?**
> | Pandas | PySpark |
> | :--- | :--- |
> | Single machine | Distribuido (cluster) |
> | Datos en memoria | Lazy evaluation |
> | MB a pocos GB | GB a TB+ |
> | Ejecuci√≥n inmediata | Necesita acci√≥n para ejecutar |

**P: ¬øQu√© es lazy evaluation en Spark?**
> Las transformaciones (`filter`, `select`, `withColumn`) no se ejecutan inmediatamente. Spark construye un DAG y solo ejecuta cuando se llama una acci√≥n (`count`, `show`, `write`). Esto permite optimizar el plan de ejecuci√≥n.

**P: ¬øC√≥mo manejas data skew en Spark?**
> 1. **Salting**: Agregar sufijo aleatorio al key, reparticionar, luego re-agregar
> 2. **Broadcast join**: Si una tabla es peque√±a, usar `broadcast()`
> 3. **AQE (Adaptive Query Execution)**: Spark lo maneja autom√°ticamente
> 4. **Repartition**: Redistribuir datos expl√≠citamente

---

## 3. üèóÔ∏è Arquitectura y Dise√±o de Sistemas

**P: Dise√±a un pipeline de datos para una empresa de e-commerce.**
> ```mermaid
> graph TD
>     A["API Orders"] --> B["Kafka"]
>     C["Base Productos"] --> D["CDC (Debezium)"]
>     D --> B
>     B --> E["Spark Streaming"]
>     E --> F["Bronze (S3/Delta)"]
>     F --> G["Silver (Limpieza)"]
>     G --> H["Gold (Star Schema)"]
>     H --> I["Snowflake/BigQuery"]
>     I --> J["Dashboards BI"]
>     I --> K["ML Models"]
> ```

**P: ¬øQu√© es ETL vs ELT? ¬øCu√°ndo usar cada uno?**
> - **ETL**: Transformar antes de cargar. Cuando el destino tiene c√≥mputo limitado (on-prem)
> - **ELT**: Cargar crudo, transformar en destino. Cuando tienes un warehouse potente (Snowflake, BigQuery) y quieres mantener datos crudos

**P: Explica la arquitectura Medallion.**
> - **Bronze**: Datos crudos, append-only, copia fiel de la fuente
> - **Silver**: Datos limpios, tipados, deduplicados, validados
> - **Gold**: Datos agregados, modelos dimensionales, listos para negocio

**P: ¬øQu√© es idempotencia y por qu√© es importante?**
> Un pipeline idempotente produce el mismo resultado sin importar cu√°ntas veces se ejecute. Es fundamental para:
> - Re-ejecuci√≥n segura en caso de fallos
> - Backfill de datos hist√≥ricos
> - Evitar duplicados en la carga

---

## 4. üì® Streaming

**P: ¬øAt-most-once vs At-least-once vs Exactly-once?**
> | Sem√°ntica | P√©rdida | Duplicados | Uso |
> | :--- | :--- | :--- | :--- |
> | At-most-once | Posible | No | M√©tricas no cr√≠ticas |
> | At-least-once | No | Posible | La mayor√≠a de casos + idempotencia |
> | Exactly-once | No | No | Transacciones financieras |

**P: ¬øC√≥mo garantizas exactly-once en Kafka?**
> 1. Productor: `enable.idempotence=true`, `acks=all`
> 2. Consumidor: Commit manual despu√©s de procesar
> 3. Transacciones: `transactional.id` para atomic writes

---

## 5. üß© Escenarios Pr√°cticos

**P: Tienes un pipeline que carga 1 bill√≥n de filas diarias. ¬øC√≥mo lo dise√±as?**
> 1. **Carga incremental**: Solo procesar datos nuevos o cambiados (CDC)
> 2. **Particionado**: Por fecha para limitar el scan
> 3. **Formato columnar**: Parquet o ORC
> 4. **Procesamiento distribuido**: Spark con cluster autoscalable
> 5. **Compresi√≥n**: Snappy o Zstandard
> 6. **Monitoreo**: Alertas de volumen, frescura y calidad

**P: Un dashboard muestra datos incorrectos. ¬øC√≥mo investigas?**
> 1. ¬øCu√°ndo empez√≥? ‚Üí Revisar historial de cambios
> 2. ¬øEs el query del dashboard o los datos? ‚Üí Consultar tabla directamente
> 3. Trazar el linaje hacia atr√°s: Gold ‚Üí Silver ‚Üí Bronze ‚Üí Fuente
> 4. ¬øCambi√≥ el schema? ¬øLlegaron datos duplicados?
> 5. Comparar conteos entre capas
> 6. Revisar logs del pipeline de esa fecha

---

## 6. üí° Preguntas de Comportamiento

**P: Cu√©ntame sobre un pipeline complejo que dise√±aste.**
> Estructura tu respuesta con **STAR**: Situaci√≥n, Tarea, Acci√≥n, Resultado.

**P: ¬øC√≥mo manejas prioridades conflictivas?**
> Evaluar impacto al negocio, comunicar con stakeholders, documentar trade-offs.

**P: ¬øC√≥mo te mantienes actualizado en el campo?**
> Blogs (Data Engineering Weekly), conferencias, podcasts, proyectos personales, comunidades.

---

## üß≠ Navegaci√≥n

Vuelve al [[√çndice Data Engineering|√çndice]]
Relacionado: [[SQL Cheatsheet|SQL]] | [[Apache Spark|Spark]] | [[Apache Kafka|Kafka]]
