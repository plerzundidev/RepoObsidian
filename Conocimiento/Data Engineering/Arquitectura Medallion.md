# ü•á Arquitectura Medallion (Bronze / Silver / Gold)

## 1. üß† Visi√≥n General

```mermaid
graph LR
    A["üì• Fuentes"] --> B["ü•â Bronze (Raw)"]
    B --> C["ü•à Silver (Cleaned)"]
    C --> D["ü•á Gold (Business)"]
    D --> E["üìä BI / Dashboards"]
    D --> F["ü§ñ ML / AI"]
    D --> G["üìã Reportes"]
```

| Capa | Alias | Prop√≥sito | Formato | Qui√©n consume |
| :--- | :--- | :--- | :--- | :--- |
| ü•â **Bronze** | Raw, Landing | Datos crudos tal cual | JSON, CSV, Parquet | Data Engineers |
| ü•à **Silver** | Cleaned, Curated | Limpio, tipado, deduplicado | Delta, Parquet, Iceberg | Data Engineers, Analysts |
| ü•á **Gold** | Business, Aggregated | Modelos dimensionales, KPIs | Delta, Parquet, Iceberg | Analysts, Stakeholders |

---

## 2. ü•â Capa Bronze (Raw)

> [!NOTE] Principio
> **Nunca modificar datos en Bronze.** Es tu copia fiel de la fuente. Append-only.

```python
# Ingesta Bronze: guardar JSON crudo
from pyspark.sql import functions as F

df_raw = spark.readStream \
    .format("cloudFiles") \
    .option("cloudFiles.format", "json") \
    .option("cloudFiles.schemaLocation", "/schemas/ventas_raw") \
    .load("s3://landing/ventas/")

df_bronze = df_raw \
    .withColumn("_ingestion_timestamp", F.current_timestamp()) \
    .withColumn("_source_file", F.input_file_name()) \
    .withColumn("_batch_id", F.lit("batch_2026-02-12"))

df_bronze.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/checkpoints/bronze_ventas") \
    .toTable("bronze.ventas")
```

### Campos Meta obligatorios en Bronze
| Campo | Descripci√≥n |
| :--- | :--- |
| `_ingestion_timestamp` | Cu√°ndo se ingiri√≥ el dato |
| `_source_file` | Archivo o API de origen |
| `_batch_id` | Identificador del batch |

---

## 3. ü•à Capa Silver (Cleaned)

```python
from pyspark.sql import functions as F

df_bronze = spark.table("bronze.ventas")

df_silver = df_bronze \
    .dropDuplicates(["venta_id"]) \
    .filter(F.col("monto").isNotNull()) \
    .filter(F.col("monto") > 0) \
    .withColumn("fecha", F.to_date("fecha_str", "yyyy-MM-dd")) \
    .withColumn("monto", F.col("monto").cast("decimal(12,2)")) \
    .withColumn("email", F.lower(F.trim(F.col("email")))) \
    .withColumn("region", F.upper(F.col("region"))) \
    .withColumn("_cleaned_timestamp", F.current_timestamp()) \
    .drop("_source_file", "_batch_id", "fecha_str")

# Escribir Silver (MERGE para idempotencia)
from delta.tables import DeltaTable

if DeltaTable.isDeltaTable(spark, "/delta/silver/ventas"):
    delta_silver = DeltaTable.forPath(spark, "/delta/silver/ventas")
    delta_silver.alias("target") \
        .merge(df_silver.alias("source"), "target.venta_id = source.venta_id") \
        .whenMatchedUpdateAll() \
        .whenNotMatchedInsertAll() \
        .execute()
else:
    df_silver.write.format("delta").save("/delta/silver/ventas")
```

### Validaciones Silver
```python
# Checks autom√°ticos post-transformaci√≥n
assert df_silver.filter(F.col("monto") <= 0).count() == 0, "Montos negativos"
assert df_silver.filter(F.col("fecha").isNull()).count() == 0, "Fechas nulas"
assert df_silver.select("venta_id").distinct().count() == df_silver.count(), "Duplicados"
```

---

## 4. ü•á Capa Gold (Business)

```sql
-- Tabla Gold: Star Schema optimizado para BI
CREATE TABLE gold.fact_ventas_diarias AS
SELECT 
    d.fecha_id,
    p.producto_id,
    c.cliente_id,
    t.tienda_id,
    COUNT(*) AS num_transacciones,
    SUM(s.monto) AS monto_total,
    AVG(s.monto) AS ticket_promedio,
    SUM(s.cantidad) AS unidades_vendidas
FROM silver.ventas s
JOIN gold.dim_fecha d ON s.fecha = d.fecha
JOIN gold.dim_producto p ON s.producto_id = p.producto_id
JOIN gold.dim_cliente c ON s.cliente_id = c.cliente_id
JOIN gold.dim_tienda t ON s.tienda_id = t.tienda_id
GROUP BY d.fecha_id, p.producto_id, c.cliente_id, t.tienda_id;

-- Vista Gold para KPIs ejecutivos
CREATE VIEW gold.kpi_ventas_mensual AS
SELECT 
    DATE_TRUNC('month', fecha) AS mes,
    SUM(monto_total) AS revenue,
    SUM(num_transacciones) AS transacciones,
    SUM(monto_total) / SUM(num_transacciones) AS ticket_promedio,
    SUM(monto_total) - LAG(SUM(monto_total)) OVER (ORDER BY DATE_TRUNC('month', fecha)) AS crecimiento
FROM gold.fact_ventas_diarias f
JOIN gold.dim_fecha d ON f.fecha_id = d.fecha_id
GROUP BY DATE_TRUNC('month', fecha);
```

---

## 5. üìÇ Estructura de Directorios

```
data-lake/
‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îú‚îÄ‚îÄ ventas/        (particionado por fecha de ingesta)
‚îÇ   ‚îú‚îÄ‚îÄ clientes/
‚îÇ   ‚îî‚îÄ‚îÄ productos/
‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îú‚îÄ‚îÄ ventas/        (particionado por fecha del evento)
‚îÇ   ‚îú‚îÄ‚îÄ clientes/      (SCD-2)
‚îÇ   ‚îî‚îÄ‚îÄ productos/
‚îú‚îÄ‚îÄ gold/
‚îÇ   ‚îú‚îÄ‚îÄ dim_fecha/
‚îÇ   ‚îú‚îÄ‚îÄ dim_cliente/
‚îÇ   ‚îú‚îÄ‚îÄ dim_producto/
‚îÇ   ‚îú‚îÄ‚îÄ fact_ventas/
‚îÇ   ‚îî‚îÄ‚îÄ met_kpis/
‚îú‚îÄ‚îÄ checkpoints/
‚îî‚îÄ‚îÄ schemas/
```

---

## 6. ‚úÖ Checklist por Capa

| Validaci√≥n | Bronze | Silver | Gold |
| :--- | :--- | :--- | :--- |
| Schema enforcement | ‚ùå | ‚úÖ | ‚úÖ |
| Deduplicaci√≥n | ‚ùå | ‚úÖ | ‚úÖ |
| Nulos tratados | ‚ùå | ‚úÖ | ‚úÖ |
| Tipos correctos | ‚ùå | ‚úÖ | ‚úÖ |
| Negocio validado | ‚ùå | ‚ùå | ‚úÖ |
| Aggregaciones | ‚ùå | ‚ùå | ‚úÖ |
| PII enmascarada | ‚ùå | ‚ö†Ô∏è | ‚úÖ |

---

## üß≠ Navegaci√≥n

Vuelve al [[√çndice Data Engineering|√çndice]]
Relacionado: [[Data Modeling|Modelado]] | [[Databricks|Databricks]] | [[Apache Iceberg|Iceberg]]
